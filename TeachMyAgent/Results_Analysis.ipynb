{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "This notebook is made to help analysing the results produced by TeachMyAgent's experiments. Using this, one can reproduce the figures we provide in our [paper](https://arxiv.org/abs/2103.09815), as well as the videos and gifs we show on our [website](https://sites.google.com/view/teachmyagent). \n",
    "\n",
    "## How to use this notebook\n",
    "This notebook is broken down into 5 sections:\n",
    "- **Imports**: import needed packages.\n",
    "- **Load Data**: load results produced by experiments and format them (e.g. calculate percentage of mastered tasks).\n",
    "- **Plot definitions**: define all the plot functions we provide (including video generation from learned policies).\n",
    "- **Experiment graphs**: use the previously defined functions to generate the different figures we show in our paper.\n",
    "- **Test tasks analysis**: analyse the performance of Deep RL students on test sets (e.g. plot test sets along with performance or use learned policy in a particular task). We also add to this section the use of our functions showing the curriculum generated by the different ACL methods.\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pylab\n",
    "import seaborn as sns\n",
    "import scipy.stats as sp\n",
    "import pickle\n",
    "import TeachMyAgent.teachers.utils.plot_utils as plotter\n",
    "import imageio\n",
    "from scipy.spatial import distance\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import copy\n",
    "import scipy.stats as ss\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.colorbar as cbar\n",
    "from matplotlib.patches import Ellipse, Rectangle\n",
    "import argparse\n",
    "import math\n",
    "from TeachMyAgent.run_utils.environment_args_handler import EnvironmentArgsHandler\n",
    "import TeachMyAgent.students.test_policy as test_policy\n",
    "from TeachMyAgent.students.run_logs_util import get_run_logs\n",
    "from TeachMyAgent.teachers.teacher_controller import param_vec_to_param_dict, param_dict_to_param_vec\n",
    "import re\n",
    "\n",
    "DIV_LINE_WIDTH = 50\n",
    "print(np.__version__)\n",
    "print(sys.executable)\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_datasets(rootdir, name_filter=None, rename_labels=False):\n",
    "    global default_colors_palette\n",
    "    _, models_list, _ = next(os.walk(rootdir))\n",
    "    print(models_list)\n",
    "    for dir_name in models_list.copy():\n",
    "        if \"ignore\" in dir_name:\n",
    "            models_list.remove(dir_name)\n",
    "        if name_filter is not None and name_filter not in dir_name:\n",
    "            models_list.remove(dir_name)\n",
    "            \n",
    "    # setting per-model type colors\n",
    "    if len(per_model_colors) == 0 and  len(models_list) > len(default_colors_palette):\n",
    "        default_colors_palette = sns.color_palette(\"hls\", len(models_list))\n",
    "        \n",
    "    for i,m_name in enumerate(models_list):\n",
    "        for m_type, m_color in per_model_colors.items():\n",
    "            if m_type in m_name:\n",
    "                colors[m_name] = m_color\n",
    "        if m_name not in colors:\n",
    "            colors[m_name] = default_colors_palette[i]\n",
    "            \n",
    "        print(\"extracting data for {}...\".format(m_name))\n",
    "        m_id = m_name\n",
    "        models_saves[m_id] = OrderedDict()\n",
    "        models_saves[m_id]['data'] = get_run_logs(rootdir+m_name, min_len=0)\n",
    "        print(\"done\")\n",
    "        if m_name not in labels:\n",
    "            if not rename_labels:\n",
    "                labels[m_name] = m_name\n",
    "            else:\n",
    "                if 'ADR' in m_name:\n",
    "                    labels[m_name] = 'ADR'\n",
    "                elif 'ALP-GMM' in m_name:\n",
    "                    labels[m_name] = 'ALP-GMM'\n",
    "                elif 'Random' in m_name:\n",
    "                    labels[m_name] = 'Random'\n",
    "                elif 'Covar-GMM' in m_name:\n",
    "                    labels[m_name] = 'Covar-GMM'\n",
    "                elif 'RIAC' in m_name:\n",
    "                    labels[m_name] = 'RIAC'\n",
    "                elif 'GoalGAN' in m_name:\n",
    "                    labels[m_name] = 'GoalGAN'\n",
    "                elif 'Self-Paced' in m_name:\n",
    "                    labels[m_name] = 'Self-Paced'\n",
    "                elif 'Setter-Solver' in m_name:\n",
    "                    labels[m_name] = 'Setter-Solver'\n",
    "                elif 'UPPER_BASELINE' in m_name:\n",
    "                    labels[m_name] = 'UPPER_BASELINE'\n",
    "                else:\n",
    "                    labels[m_name] = m_name\n",
    "labels = OrderedDict()\n",
    "default_colors_palette = sns.color_palette()\n",
    "per_model_colors = OrderedDict([('ALP-GMM', default_colors_palette[0]),\n",
    "                                ('Covar-GMM', default_colors_palette[1]),\n",
    "                                ('ADR', default_colors_palette[2]),\n",
    "                                ('Random', default_colors_palette[3]),\n",
    "                                ('RIAC', default_colors_palette[4]),\n",
    "                                ('GoalGAN', default_colors_palette[5]),\n",
    "                                ('Self-Paced', default_colors_palette[6]),\n",
    "                                ('Setter-Solver', default_colors_palette[7]),\n",
    "                                ('UPPER_BASELINE', default_colors_palette[8])])\n",
    "\n",
    "models_saves = OrderedDict()\n",
    "colors = OrderedDict()\n",
    "\n",
    "##### MODIFY THIS TO POINT TO YOUR DATA FOLDER #####\n",
    "data_folder = \"ACL_bench/data/BENCHMARK/\"\n",
    "##### MODIFY THIS TO POINT TO YOUR DATA FOLDER #####\n",
    "\n",
    "get_datasets(data_folder, rename_labels=True)\n",
    "# get_datasets(data_folder, rename_labels=True, name_filter=\"parkour\") # You can also add filters\n",
    "\n",
    "# order runs for legend order as in per_models_colors, with corresponding colors\n",
    "if len(per_model_colors) > 0:\n",
    "    ordered_labels = OrderedDict()\n",
    "    for teacher_type in per_model_colors.keys():\n",
    "        for k,v in labels.items():\n",
    "            if teacher_type in k:\n",
    "                ordered_labels[k] = v\n",
    "    labels = ordered_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle baseline Random teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "default_configuration = \"no\"\n",
    "configurations_to_add = [\"minimal\", \"maximal\"]\n",
    "new_expes_to_add = {}\n",
    "for expe_id in models_saves:\n",
    "    if \"profiling_benchmark_stumps_Random\" in expe_id:\n",
    "        for new_config in configurations_to_add:\n",
    "            new_expe_id = expe_id.replace(\"allow_expert_knowledge_\" + default_configuration,\n",
    "                                          \"allow_expert_knowledge_\" + new_config)\n",
    "            new_expes_to_add[new_expe_id] = OrderedDict()\n",
    "            new_expes_to_add[new_expe_id]['data'] = copy.copy(models_saves[expe_id]['data'])\n",
    "            labels[new_expe_id] = labels[expe_id]\n",
    "            colors[new_expe_id] = colors[expe_id]\n",
    "models_saves.update(new_expes_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Upper Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_to_add = [\"1\", \"2\", \"3\", \"4\"]\n",
    "new_expes_to_add = {}\n",
    "for expe_id in models_saves:\n",
    "    if \"UPPER_BASELINE\" in expe_id:\n",
    "        for criterion in criteria_to_add:\n",
    "            new_expe_id = expe_id.replace(\"UPPER_BASELINE\",\n",
    "                                          \"UPPER_BASELINE_criteria_\" + criterion)\n",
    "            new_expes_to_add[new_expe_id] = OrderedDict()\n",
    "            new_expes_to_add[new_expe_id]['data'] = copy.copy(models_saves[expe_id]['data'])\n",
    "            labels[new_expe_id] = labels[expe_id]\n",
    "            colors[new_expe_id] = colors[expe_id]\n",
    "models_saves.update(new_expes_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge experiments by teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some experiments (e.g. on parkour or criteria 5) were broken down into multiple experiments. In order to analyze them, the results must be merged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL: IT NEEDS RENAME_LABELS SET TO TRUE\n",
    "def merge_experiments_by_teacher(experiment_name, result_name=None):\n",
    "    new_saves = OrderedDict()\n",
    "    anything_token = \"*\"\n",
    "    anything_pattern = \"[a-zA-z0-9\\-.]*\"\n",
    "    experiment_name_regex_pattern = experiment_name.replace(anything_token, anything_pattern)\n",
    "    regex = re.compile(experiment_name_regex_pattern)\n",
    "    for expe_id in models_saves:\n",
    "        if regex.match(expe_id):\n",
    "            associated_label = labels[expe_id]\n",
    "            if result_name is None:\n",
    "                new_expe_name = experiment_name.replace('*', '').replace('|', ',') + \"_\" + associated_label\n",
    "            else:\n",
    "                new_expe_name = result_name.replace('{LABEL}', associated_label)\n",
    "\n",
    "            if models_saves[expe_id]['data'] is not None:\n",
    "                if new_expe_name not in new_saves:\n",
    "                    new_saves[new_expe_name] = OrderedDict()\n",
    "                    new_saves[new_expe_name]['data'] = copy.copy(models_saves[expe_id]['data'])\n",
    "                    labels[new_expe_name] = associated_label\n",
    "                    colors[new_expe_name] = colors[expe_id]\n",
    "                else:\n",
    "                    new_saves[new_expe_name]['data'].extend(copy.copy(models_saves[expe_id]['data']))\n",
    "    models_saves.update(new_saves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_experiments_by_teacher(\"*benchmark_parkour*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_experiments_by_teacher(\"*profiling_benchmark_stumps_*_criteria_5*allow_expert_knowledge_no*\", \n",
    "                             result_name=\"profiling_benchmark_stumps_{LABEL}_criteria_5_allow_expert_knowledge_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_experiments_by_teacher(\"*profiling_benchmark_stumps_*_criteria_5*allow_expert_knowledge_minimal*\", \n",
    "                             result_name=\"profiling_benchmark_stumps_{LABEL}_criteria_5_allow_expert_knowledge_minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_experiments_by_teacher(\"*profiling_benchmark_stumps_*_criteria_5*allow_expert_knowledge_maximal*\", \n",
    "                             result_name=\"profiling_benchmark_stumps_{LABEL}_criteria_5_allow_expert_knowledge_maximal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute mastered tasks percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute \"% of Mastered tasks\" metric: percentage of test tasks (over a test set of 100 tasks) on which the agent obtained an episodic reward greater than a threshold (230)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mastered_thr = 230\n",
    "for i,(m_id,label) in enumerate(labels.items()):\n",
    "    print(m_id)\n",
    "    runs_data = models_saves[m_id]['data']\n",
    "    #collect raw perfs\n",
    "    print(\"Seeds : \" + str(len(runs_data)))\n",
    "    for r,run in enumerate(runs_data):\n",
    "        models_saves[m_id]['data'][r]['nb_mastered'] = []\n",
    "        models_saves[m_id]['data'][r]['avg_pos_rewards'] = []\n",
    "        models_saves[m_id]['data'][r]['local_rewards'] = []\n",
    "        if 'env_test_rewards' in run:\n",
    "            size_test_set = int(len(run['env_test_rewards'])/len(run['evaluation return']))\n",
    "            for j in range(len(run['evaluation return'])):#max_epoch):\n",
    "                test_data = np.array(run['env_test_rewards'][j*size_test_set:(j+1)*(size_test_set)])\n",
    "                nb_mastered = len(np.where(test_data > mastered_thr)[0])\n",
    "                models_saves[m_id]['data'][r]['nb_mastered'].append((nb_mastered/size_test_set)*100)\n",
    "        else:\n",
    "            print(\"Skipping seed {}\".format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute best seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best seed of each experiment. This is then used to plot policies and analyze test set performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_seed(expe_name, metric=\"evaluation return\"):\n",
    "    best_seed = -1\n",
    "    best_seed_value = -1000\n",
    "    runs_data = models_saves[expe_name]['data']\n",
    "    all_values = []\n",
    "    for run in runs_data:\n",
    "        if len(run[metric]) > 0:\n",
    "            data = run[metric][-1]\n",
    "            all_values.append(data)\n",
    "            if data > best_seed_value:\n",
    "                best_seed_value = data\n",
    "                best_seed = run[\"config\"][\"seed\"]\n",
    "        else:\n",
    "            print(\"Skipping seed {}: no data\".format(run[\"config\"][\"seed\"]))\n",
    "    return best_seed, best_seed_value, np.mean(all_values), np.std(all_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_seeds = {}\n",
    "for i,(m_id,label) in enumerate(labels.items()):\n",
    "    best_seed, best_seed_value, mean, std = get_best_seed(m_id, metric=\"nb_mastered\")\n",
    "    best_seeds[m_id] = best_seed\n",
    "    print(\"Expe {0} : {1} ({2}) - Mean: {3} ({4})\".format(m_id, best_seed, best_seed_value, mean, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_shade(subplot_nb, ax,x,y,err,color,shade_color,label,\n",
    "                  y_min=None,y_max=None, legend=False, leg_size=30, leg_loc='best', title=None,\n",
    "                  ylim=[0,100], xlim=[0,40], leg_args={}, leg_linewidth=8.0, linewidth=7.0,\n",
    "                  ticksize=30, y_label='% Mastered env', label_size=30):\n",
    "    ax.locator_params(axis='x', nbins=5)\n",
    "    ax.locator_params(axis='y', nbins=5)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=ticksize)\n",
    "    ax.plot(x,y, color=color, label=label,linewidth=linewidth)\n",
    "    ax.fill_between(x,y-err,y+err,color=shade_color,alpha=0.2)\n",
    "    if legend:\n",
    "        leg = ax.legend(loc=leg_loc, fontsize=leg_size, **leg_args) #34\n",
    "        for legobj in leg.legendHandles:\n",
    "            legobj.set_linewidth(leg_linewidth)\n",
    "    ax.set_xlabel('Million steps', fontsize=label_size)\n",
    "    if subplot_nb == 0:\n",
    "        ax.set_ylabel(y_label, fontsize=label_size)\n",
    "    ax.set_xlim(xmin=xlim[0],xmax=xlim[1])\n",
    "    ax.set_ylim(bottom=ylim[0],top=ylim[1])\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=22)\n",
    "\n",
    "def plot_all_and_median(subplot_nb, ax,x,ys,color,label,\n",
    "                         y_min=None,y_max=None, legend=False, title=None, x_max=20, y_label='% Mastered env'):\n",
    "    ax.locator_params(axis='x', nbins=5)\n",
    "    ax.locator_params(axis='y', nbins=5)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=30)\n",
    "    min_len = 999999\n",
    "    median = np.median(ys, axis=0)\n",
    "    for k,y in enumerate(ys):\n",
    "        ax.plot(x[0:min_len],y, color=color, linewidth=1.5, alpha=0.5)\n",
    "    ax.plot(x[0:min_len],median, color=color, linewidth=7 , label=label)\n",
    "    if legend:\n",
    "        leg = ax.legend(loc='best', fontsize=25)\n",
    "    ax.set_xlabel('Million steps', fontsize=18)\n",
    "    if subplot_nb == 0:\n",
    "        ax.set_ylabel(y_label, fontsize=18)\n",
    "    ax.set_xlim(xmin=0,xmax=x_max)\n",
    "    if y_min is not None:\n",
    "        ax.set_ylim(bottom=y_min,top=y_max)\n",
    "    else:\n",
    "        ax.set_ylim(top=100)\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=22)\n",
    "        \n",
    "def get_percentiles(data, label, max_ep=100):\n",
    "    nb_zero_perf = np.count_nonzero(data[:max_ep]==0.0)\n",
    "    print('{} -> nb zeros: {}'.format(label, nb_zero_perf))\n",
    "    print('{} -> percentile: {}'.format(label, np.percentile(data[:max_ep],[25,50,75,90])))\n",
    "    \n",
    "def get_simple_welch(d1, d2):\n",
    "    return ss.ttest_ind(d1, d2, equal_var=False)\n",
    "\n",
    "def get_multistep_welch(algo_0, algo_1, epoch=0):\n",
    "    tt_test = []\n",
    "    max_values =  []\n",
    "    for i in range(min(len(algo_0[0]), len(algo_1[0]))):\n",
    "        d1 = [v[i] for v in algo_0]\n",
    "        d2 = [v[i] for v in algo_1]\n",
    "        max_values.append(max(d1 + d2))\n",
    "        tt_test.append(get_simple_welch(d1, d2))\n",
    "        \n",
    "    return tt_test, max_values\n",
    "    \n",
    "def get_welch_from_names(algo_0='amb', algo_1='rmb', epoch=0, metric='nb_mastered'):\n",
    "    print(\"algo0:{}, algo1:{}\".format(algo_0,algo_1))\n",
    "    final_explos = dict()\n",
    "    for i,(m_id,d) in enumerate(models_saves.items()):\n",
    "        if algo_0 in m_id or algo_1 in m_id:\n",
    "            is_algo_0 = algo_0 in m_id\n",
    "            final_explos[algo_0 if is_algo_0 else algo_1] = []\n",
    "            runs_data = d['data']\n",
    "            ys = []\n",
    "            if epoch != 0:\n",
    "                long_enough = True\n",
    "                for run in runs_data:\n",
    "                    if len(run[metric]) < epoch:\n",
    "                        long_enough = False\n",
    "                if not long_enough:\n",
    "                    print(\"aborting: {} not long enough\".format(m_id))\n",
    "                    return\n",
    "            for run in runs_data:\n",
    "                final_explos[algo_0 if is_algo_0 else algo_1].append(run[metric][epoch-1])\n",
    "                    \n",
    "    print('welch {}'.format(ss.ttest_ind(final_explos[algo_0], final_explos[algo_1], equal_var=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(agent_type, plot_type='shade', metric='nb_mastered', legend=True, y_min=0, y_max=100, x_max=10, \n",
    "                allow_different_sizes=False, welch=False, welch_p_threshold=0.05, _ax=None, leg_size=20):\n",
    "    if _ax is None:\n",
    "        f, ax = plt.subplots(1,1,figsize=(30,12))\n",
    "    else:\n",
    "        ax = _ax\n",
    "    ys_for_weclh = {}\n",
    "    max_y = -1\n",
    "    anything_token = \"*\"\n",
    "    anything_pattern = \"[a-zA-z0-9\\-.]*\"\n",
    "    agent_type_regex_pattern = agent_type.replace(anything_token, anything_pattern)\n",
    "    regex = re.compile(agent_type_regex_pattern)\n",
    "    for i,(m_id,label) in enumerate(labels.items()):\n",
    "        if regex.match(m_id):\n",
    "            runs_data = models_saves[m_id]['data']\n",
    "            ys = []\n",
    "            episodes = []\n",
    "            nb_seeds = len(runs_data)\n",
    "            if nb_seeds > 0:\n",
    "                for run in runs_data:  \n",
    "                    data = run[metric]\n",
    "                    if len(run['total timesteps']) > len(episodes):\n",
    "                        episodes = np.array(run['total timesteps'])\n",
    "                    ys.append(data)\n",
    "                if not allow_different_sizes:\n",
    "                    #clean data in case an expe has seeds with varying epoch number    \n",
    "                    min_len = 999999\n",
    "                    for y in ys:\n",
    "                        if len(y) < min_len:\n",
    "                            min_len = len(y)\n",
    "                    ys_same_len = np.empty((len(ys), min_len))\n",
    "                    for i in range(len(ys)):\n",
    "                        y = ys[i]\n",
    "                        for j in range(min_len):\n",
    "                            ys_same_len[i, j] = y[j]\n",
    "                    episodes = episodes[0:min_len]\n",
    "                else:\n",
    "                    full_len = max([len(y) for y in ys])\n",
    "                    ys_same_len = np.ma.empty((len(ys), full_len))\n",
    "                    ys_same_len.mask = True\n",
    "                    for i in range(len(ys)):\n",
    "                        y = ys[i]\n",
    "                        for j in range(len(y)):\n",
    "                            ys_same_len[i, j] = y[j]\n",
    "                episodes = [e/1000000 for e in episodes]\n",
    "                \n",
    "                if ys_same_len.size > 0:\n",
    "                    if welch:\n",
    "                        ys_for_weclh[m_id] = ys_same_len\n",
    "\n",
    "                    if plot_type in [\"shade\", \"shade_se\"]:\n",
    "                        stds = ys_same_len.std(axis=0)\n",
    "                        if plot_type == \"shade_se\":\n",
    "                            stds = stds / math.sqrt(nb_seeds)\n",
    "                        means = ys_same_len.mean(axis=0)\n",
    "                        max_y = max(max_y, max(means + stds))\n",
    "                        plot_with_shade(0, ax, episodes, means, stds, colors[m_id],\n",
    "                                        colors[m_id], label, leg_loc=(0,0.39), y_label=metric, leg_args={\"frameon\":False},\n",
    "                                        legend=legend,ylim=[y_min,y_max], xlim=[0,x_max], leg_size=leg_size,\n",
    "                                        ticksize=40, label_size=40)\n",
    "                    elif plot_type == \"all_and_median\":\n",
    "                        plot_all_and_median(0, ax, episodes,ys_same_len,colors[m_id],label,\n",
    "                                            title=\"{}\".format(agent_type), legend=legend, x_max=x_max,\n",
    "                                            y_min=y_min, y_max=y_max, y_label=metric)\n",
    "    if welch:\n",
    "        is_diff_significant = {}\n",
    "        i = 0\n",
    "        for expe in ys_for_weclh:\n",
    "            is_diff_significant[expe] = {}\n",
    "            j = 0\n",
    "            for expe_2 in ys_for_weclh:\n",
    "                if expe != expe_2:\n",
    "                    ttest_results, maxs = get_multistep_welch(ys_for_weclh[expe], ys_for_weclh[expe_2])\n",
    "                    k = 0\n",
    "                    max_val = max_y if max_y > 0 else max(maxs)\n",
    "                    for ttest in ttest_results:\n",
    "                        if ttest[1] < welch_p_threshold:\n",
    "                            ax.plot(k, max_val + 3*(i+j) + 10, '*', markersize=20, c=list(colors.items())[i+j][1])\n",
    "                        k += 1\n",
    "                    j += 1\n",
    "                                                    \n",
    "            if len(ys_for_weclh) / 2 == i + 1:\n",
    "                break\n",
    "            i += 1\n",
    "                    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if _ax is None:\n",
    "        f.savefig('TeachMyAgent/graphics/{0}_{1}_{2}.png'.\n",
    "                  format(agent_type.replace('*', '[]').replace('|', ','), plot_type, metric), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_comparisons(agent_type, metric='nb_mastered', y_min=0, y_max=100, x_max=20, allow_different_sizes=False, welch_p_threshold=0.05):\n",
    "    anything_token = \"*\"\n",
    "    anything_pattern = \"[a-zA-z0-9\\-.]*\"\n",
    "    agent_type_regex_pattern = agent_type.replace(anything_token, anything_pattern)\n",
    "    regex = re.compile(agent_type_regex_pattern)\n",
    "    agents_to_plot = []\n",
    "    for i,(m_id,label) in enumerate(labels.items()):\n",
    "        if regex.match(m_id):\n",
    "            agents_to_plot.append(m_id)\n",
    "                     \n",
    "    nb_columns = len(agents_to_plot) + 1\n",
    "    nb_rows = len(agents_to_plot) + 2        \n",
    "    fig = plt.figure(constrained_layout=True, figsize=(35 + 5*nb_rows, 30 + 5*nb_columns))\n",
    "    widths = [0.2 if i == 0 else 1 for i in range(nb_columns)]\n",
    "    heights = [4, 0.2] + [1 for _ in range(nb_rows - 2)]\n",
    "    gs = fig.add_gridspec(nb_rows, nb_columns, width_ratios=widths, height_ratios=heights)\n",
    "    fig.patch.set_facecolor('#f7f7f7')\n",
    "    \n",
    "    f_0_0_ax = fig.add_subplot(gs[0, :])\n",
    "    plot_curves(agent_type, metric=metric, plot_type=\"shade_se\", welch=False, _ax=f_0_0_ax, y_min=y_min, y_max=y_max, x_max=x_max, leg_size=40, allow_different_sizes=allow_different_sizes)\n",
    "    for i in range(len(agents_to_plot) + 1):\n",
    "        for j in range(len(agents_to_plot) + 1):\n",
    "            ax = fig.add_subplot(gs[i+1, j])\n",
    "            if i == 0 or j == 0:\n",
    "                ax.set_axis_off()\n",
    "                if i + j > 0:\n",
    "                    if i == 0:\n",
    "                        idx = j - 1\n",
    "                    else:\n",
    "                        idx = i - 1\n",
    "                    ax.text(0.5, 0.5, labels[agents_to_plot[idx]], ha=\"center\", va=\"center\", fontsize=50)\n",
    "            else:\n",
    "                if i > j:\n",
    "                    plot_curves(\"(\" + agents_to_plot[i - 1] + \"|\" + agents_to_plot[j - 1] + \")\", plot_type=\"shade_se\",\n",
    "                                metric=metric, welch=True, welch_p_threshold=welch_p_threshold, _ax=ax, \n",
    "                                y_min=y_min, y_max=y_max, x_max=x_max, allow_different_sizes=allow_different_sizes)\n",
    "                else:\n",
    "                    ax.set_axis_off()\n",
    "                    \n",
    "    plt.savefig('TeachMyAgent/graphics/comparisons_{}_{}.png'.format(\n",
    "        agent_type.replace('*', '[]').replace('|', ','), metric.replace(\" \", \"_\")), \n",
    "        facecolor='#f7f7f7', edgecolor='none', bbox_inches='tight', dpi=100)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL: IT NEEDS RENAME_LABELS SET TO TRUE AS WELL AS PER MODEL COLORS ACTIVATED\n",
    "def generate_profile_chart(expe_template_name=\"(*profiling_benchmark_stumps_*|UPPER_BASELINE)_criteria_(1|2|3|4|5)_allow_expert_knowledge_(no|minimal|maximal)$\", \n",
    "                           baseline_teacher=\"Random\", list_of_teachers=None, tick_step=0.5, timestep=-1):\n",
    "    anything_token = \"*\"\n",
    "    anything_pattern = \"[a-zA-z0-9\\-.]*\"\n",
    "    experiment_name_regex_pattern = expe_template_name.replace(anything_token, anything_pattern)\n",
    "    name_regex = re.compile(experiment_name_regex_pattern)\n",
    "    criteria_regex = re.compile(anything_pattern + \"_criteria_[0-9]\")\n",
    "    \n",
    "    figname = \"benchmark_profiling{}\".format('_' + '_'.join(list_of_teachers) if list_of_teachers is not None else '')\n",
    "    if timestep != -1:\n",
    "        figname += \"_timestep-\" + str(timestep)\n",
    "    \n",
    "    criterion_label = [\n",
    "        \"Mostly unfeasible\\n task space\",\n",
    "        \"Mostly trivial\\n task space\",\n",
    "        \"Student that\\n forgets\",\n",
    "        \"Rugged\\n difficulty\",\n",
    "        \"Variety of\\n students\"\n",
    "    ]\n",
    "    \n",
    "    ek_types = [\"no\", \"low\", \"high\"]\n",
    "    \n",
    "    linestyle_tuple = [\n",
    "     ('solid',                 (0, ())),\n",
    "        \n",
    "     ('loosely dotted',        (0, (1, 1))),\n",
    "     ('dotted',                (0, (1, 0.5))),\n",
    "\n",
    "     ('loosely dashed',        (0, (5, 1.5))),\n",
    "     ('dashed',                (0, (5, 1))),\n",
    "     ('densely dashed',        (0, (5, 0.5))),\n",
    "    \n",
    "     ('loosely dashdotted',    (0, (3, 1, 1, 1))),   \n",
    "     ('dashdotted',            (0, (3, 0.5, 1, 0.5)))\n",
    "    ]\n",
    "    \n",
    "    linestyles = {}\n",
    "\n",
    "    df_columns = [ek + \"_\" + criterion for criterion in criterion_label.copy() for ek in ek_types.copy()]\n",
    "    df_indexes = list(set(labels.values()))\n",
    "    raw_results = pd.DataFrame(\n",
    "        index=df_indexes, \n",
    "        columns=df_columns)\n",
    "    processed_results = pd.DataFrame(\n",
    "        index=df_indexes, \n",
    "        columns=df_columns)\n",
    "    \n",
    "    ### Get results ###\n",
    "    linestyle_iterrator = 0\n",
    "    for expe_id in models_saves:\n",
    "        if name_regex.match(expe_id):\n",
    "            current_label = labels[expe_id]\n",
    "            if list_of_teachers is not None and current_label not in list_of_teachers:\n",
    "                break\n",
    "                \n",
    "            if current_label not in linestyles and \"UPPER_BASELINE\" not in current_label:\n",
    "                linestyles[current_label] = linestyle_tuple[linestyle_iterrator][1]\n",
    "                linestyle_iterrator += 1\n",
    "            \n",
    "            # Get column prefix\n",
    "            if \"allow_expert_knowledge_no\" in expe_id:\n",
    "                prefix = \"no_\"\n",
    "            elif \"allow_expert_knowledge_minimal\" in expe_id:\n",
    "                prefix = \"low_\"\n",
    "            elif \"allow_expert_knowledge_maximal\" in expe_id:\n",
    "                prefix = \"high_\"\n",
    "            else:\n",
    "                raise Exception()\n",
    "                \n",
    "            # Get criteria\n",
    "            match = criteria_regex.match(expe_id)\n",
    "            criteria_id = match.group()[-1]\n",
    "            column = prefix + criterion_label[int(criteria_id) - 1]\n",
    "                \n",
    "            current_criteria_values = []\n",
    "            nb_seeds = len(models_saves[expe_id][\"data\"])\n",
    "            for seed in range(nb_seeds):\n",
    "                seed_values_array = models_saves[expe_id][\"data\"][seed][\"nb_mastered\"]\n",
    "                if timestep != -1 and len(seed_values_array) > timestep and not \"UPPER_BASELINE\" in current_label:\n",
    "                    seed_value = seed_values_array[timestep]\n",
    "                else:\n",
    "                    seed_value = seed_values_array[-1]\n",
    "                current_criteria_values.append(seed_value)\n",
    "            \n",
    "            mean_result = np.mean(current_criteria_values)\n",
    "            std_result = np.std(current_criteria_values)\n",
    "            raw_results.loc[current_label][column] = mean_result\n",
    "            \n",
    "    ### Generate chart results ###\n",
    "    for index, row in raw_results.iterrows():\n",
    "        for col in raw_results.columns:\n",
    "            if index == baseline_teacher:\n",
    "                processed_results.loc[index][col] = 1\n",
    "            else:\n",
    "                baseline_val = raw_results.loc[baseline_teacher][col]\n",
    "                current_val = row[col]\n",
    "                processed_results.loc[index][col] = current_val / baseline_val\n",
    "                \n",
    "    ### Generate plot ### \n",
    "    N = len(processed_results.columns) / len(ek_types)\n",
    "    N = int(N)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * math.pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    # Initialise the plot\n",
    "    nb_columns = 2\n",
    "    nb_rows = 2      \n",
    "    fig = plt.figure(constrained_layout=True, figsize=(80, 60))\n",
    "    gs = fig.add_gridspec(nb_rows, nb_columns)\n",
    "    fig.subplots_adjust(hspace=0.05)\n",
    "    \n",
    "    for i in range(len(ek_types)):\n",
    "        current_ek_filter = ek_types[i] + \"_\"\n",
    "        if i == 0:\n",
    "            subplot_pos = gs[0, :]\n",
    "        else:\n",
    "            subplot_pos = gs[1, (i-1)%2]\n",
    "        ax = fig.add_subplot(subplot_pos, polar=True)\n",
    "        ax.patch.set_facecolor('white')\n",
    "        ax.title.set_text((ek_types[i] + \" expert knowledge\").capitalize())\n",
    "        ax.title.set_fontsize(80)\n",
    "        ax.tick_params(axis='both', which='major', pad=-50)\n",
    "\n",
    "        # Set labels\n",
    "        plt.xticks(angles[:-1], criterion_label, color='grey', size=65)\n",
    "        for label, angle in zip(ax.get_xticklabels(), np.rad2deg(angles)):\n",
    "            if angle == 90 or angle == 270:\n",
    "                label.set_horizontalalignment('center')\n",
    "            elif 90 < angle < 270:\n",
    "                label.set_horizontalalignment('right')\n",
    "            else:\n",
    "                label.set_horizontalalignment('left')\n",
    "\n",
    "        # Set tick lines\n",
    "        for line in ax.xaxis.get_gridlines():\n",
    "            line.set_color('grey')\n",
    "            line.set_alpha(0.95)\n",
    "            line.set_linestyle(':')\n",
    "            line.set_linewidth(2)\n",
    "\n",
    "        for line in ax.yaxis.get_gridlines():\n",
    "            line.set_color('grey')\n",
    "            line.set_alpha(0.95)\n",
    "            line.set_linestyle(':')\n",
    "            line.set_linewidth(2)\n",
    "\n",
    "        # Change radar's background\n",
    "        max_val = max(processed_results.filter(like=current_ek_filter).max())\n",
    "        ticks = []\n",
    "        fill_values = np.linspace(0, 2*np.pi, 100)\n",
    "        for j in np.arange(0, max_val+tick_step, tick_step):\n",
    "            ticks.append(j)\n",
    "            ax.fill(fill_values, [j,] * 100, color='k', alpha=0.05)\n",
    "\n",
    "        # Draw ylabels\n",
    "        ax.set_rlabel_position(0)\n",
    "        plt.yticks(ticks, [str(t) for t in ticks], color='k', alpha=0.75, fontsize=55, ha=\"center\")\n",
    "        plt.ylim(0,max_val + tick_step)\n",
    "        ax.set_axisbelow(False)\n",
    "        \n",
    "        for index, row in processed_results.filter(like=current_ek_filter).iterrows():\n",
    "            values = list(row.values)\n",
    "            values += values[:1]\n",
    "            if index == \"UPPER_BASELINE\":\n",
    "                ax.plot(angles, values, \"*\", color='red', markersize=30)\n",
    "            else:\n",
    "                # Plot data\n",
    "                ax.plot(angles, values, linewidth=10, linestyle=linestyles[index], \n",
    "                        color=per_model_colors[index], \n",
    "                        label=index + (\" (baseline)\" if index == baseline_teacher else \"\"))\n",
    "                # Fill area\n",
    "                ax.fill(angles, values, 'b', alpha=0.1, color=per_model_colors[index])\n",
    "\n",
    "        if i == 0:\n",
    "            from matplotlib.lines import Line2D\n",
    "            handles = {}\n",
    "            legend = ax.legend(loc=(1.5,0.2), fontsize=65)\n",
    "            for legobj in legend.legendHandles:\n",
    "                current_legobj = copy.copy(legobj)\n",
    "                current_legobj.set_linewidth(15)\n",
    "                handles[current_legobj._label] = current_legobj\n",
    "                \n",
    "            del legend\n",
    "            # CHANGE THIS IF YOU ADD A NEW TEACHER #\n",
    "            ax.legend(handles=[\n",
    "                handles['Random (baseline)'],\n",
    "                handles['ALP-GMM'],\n",
    "                handles['Covar-GMM'],\n",
    "                handles['RIAC'],\n",
    "                handles['Self-Paced'],\n",
    "                Line2D([], [], linestyle='', label=\"\"),\n",
    "                Line2D([], [], linestyle='', label='$\\it{EK}$* $\\it{required}$:'),\n",
    "                handles['ADR'],\n",
    "                handles['GoalGAN'],\n",
    "                handles['Setter-Solver'],\n",
    "            ], loc=(1.5,0.1), fontsize=65)\n",
    "    \n",
    "    plt.savefig('TeachMyAgent/graphics/{}.png'.format(figname), bbox_inches='tight', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL: IT NEEDS RENAME_LABELS SET TO TRUE AS WELL AS PER MODEL COLORS ACTIVATED\n",
    "def barplot_annotate_brackets(num1, num2, text, center, height, height_index, yerr=None, barh=.05):\n",
    "    ref_y = max(height)\n",
    "\n",
    "    lx, ly = center[num1], height[num1]\n",
    "    rx, ry = center[num2], height[num2]\n",
    "\n",
    "    if yerr:\n",
    "        ly += yerr[num1]\n",
    "        ry += yerr[num2]\n",
    "\n",
    "    ax_y0, ax_y1 = plt.gca().get_ylim()\n",
    "    barh *= (ax_y1 - ax_y0)\n",
    "\n",
    "    y = max(height) + height_index\n",
    "\n",
    "    barx = [lx, lx, rx, rx]\n",
    "    bary = [y, y+barh, y+barh, y]\n",
    "    mid = ((lx+rx)/2, y+barh)\n",
    "\n",
    "    plt.plot(barx, bary, c='black', linestyle='solid', linewidth=3)#(0, (1, 0.5)\n",
    "\n",
    "    kwargs = dict(ha='center', va='bottom')\n",
    "\n",
    "    plt.text(*mid, text, **kwargs)\n",
    "    \n",
    "def generate_comparison_bars(expe_template_name=\"*profiling_benchmark_stumps_*_criteria_(1|2|3|4|5)_allow_expert_knowledge_(no|minimal|maximal)$\", \n",
    "                             list_of_teachers=None, timestep=-1):\n",
    "    anything_token = \"*\"\n",
    "    anything_pattern = \"[a-zA-z0-9\\-.]*\"\n",
    "    experiment_name_regex_pattern = expe_template_name.replace(anything_token, anything_pattern)\n",
    "    name_regex = re.compile(experiment_name_regex_pattern)\n",
    "    criteria_regex = re.compile(anything_pattern + \"_criteria_[0-9]\")\n",
    "    \n",
    "    figname = \"benchmark_bars{}\".format('_' + '_'.join(list_of_teachers) if list_of_teachers is not None else '')\n",
    "    if timestep != -1:\n",
    "        figname += \"_timestep-\" + str(timestep)\n",
    "    \n",
    "    criterion_label = [\n",
    "        \"Mostly\\n unfeasible\\n task space\",\n",
    "        \"Mostly\\n trivial\\n task space\",\n",
    "        \"Student\\n that\\n forgets\",\n",
    "        \"Rugged\\n difficulty\",\n",
    "        \"Variety\\n of\\n students\"\n",
    "    ]\n",
    "    \n",
    "    ek_types = [\"no\", \"low\", \"high\"]\n",
    "\n",
    "    raw_results = {}\n",
    "    \n",
    "    for ek in ek_types:\n",
    "        raw_results[ek] = {}\n",
    "        for criterion in criterion_label:\n",
    "            raw_results[ek][criterion] = {}\n",
    "    \n",
    "    ### Get results ###\n",
    "    linestyle_iterrator = 0\n",
    "    for expe_id in models_saves:\n",
    "        if name_regex.match(expe_id):\n",
    "            current_label = labels[expe_id]\n",
    "            if list_of_teachers is not None and current_label not in list_of_teachers:\n",
    "                break\n",
    "            \n",
    "            # Get column prefix\n",
    "            if \"allow_expert_knowledge_no\" in expe_id:\n",
    "                results_index = \"no\"\n",
    "            elif \"allow_expert_knowledge_minimal\" in expe_id:\n",
    "                results_index = \"low\"\n",
    "            elif \"allow_expert_knowledge_maximal\" in expe_id:\n",
    "                results_index = \"high\"\n",
    "            else:\n",
    "                raise Exception()                   \n",
    "            # Get criteria\n",
    "            match = criteria_regex.match(expe_id)\n",
    "            criteria_id = match.group()[-1]\n",
    "            current_criterion = criterion_label[int(criteria_id) - 1]\n",
    "                \n",
    "            current_criteria_values = []\n",
    "            nb_seeds = len(models_saves[expe_id][\"data\"])\n",
    "            for seed in range(nb_seeds):\n",
    "                seed_values_array = models_saves[expe_id][\"data\"][seed][\"nb_mastered\"]\n",
    "                if timestep != -1 and len(seed_values_array) > timestep:\n",
    "                    seed_value = seed_values_array[timestep]\n",
    "                else:\n",
    "                    seed_value = seed_values_array[-1]\n",
    "                current_criteria_values.append(seed_value)\n",
    "            \n",
    "            mean_result = np.mean(current_criteria_values)\n",
    "            std_result = np.std(current_criteria_values)\n",
    "            raw_results[results_index][current_criterion][current_label] = {\n",
    "                \"mean\": mean_result,\n",
    "                \"std\": std_result,\n",
    "                \"seeds\": current_criteria_values\n",
    "            }\n",
    "\n",
    "                \n",
    "    ### Generate plot ### \n",
    "    # Initialise the plot\n",
    "    nb_columns = len(ek_types) + 1\n",
    "    nb_rows = len(criterion_label) + 1\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(100, 150))\n",
    "    widths = [0.1 if i == 0 else 1 for i in range(nb_columns)]\n",
    "    heights = [0.1 if i == 0 else 1 for i in range(nb_rows)]\n",
    "    gs = fig.add_gridspec(nb_rows, nb_columns, width_ratios=widths, height_ratios=heights)\n",
    "    fig.subplots_adjust(wspace=0.05)\n",
    "    \n",
    "    for i in range(nb_columns):\n",
    "        for j in range(nb_rows):\n",
    "            ax = fig.add_subplot(gs[j, i])\n",
    "            if i == 0 or j == 0:\n",
    "                ax.set_axis_off()\n",
    "                if i + j > 0:\n",
    "                    if i == 0:\n",
    "                        ax.text(0.5, 0.5, criterion_label[j-1], ha=\"center\", va=\"center\", fontsize=50)\n",
    "                    else:\n",
    "                        ax.text(0.5, 0.5, (ek_types[i-1] + \" expert knowledge\").capitalize(), ha=\"center\", va=\"center\", fontsize=50)\n",
    "                    \n",
    "            else:\n",
    "                current_ek_type = ek_types[i-1]\n",
    "                current_criterion = criterion_label[j-1]\n",
    "                current_expe = raw_results[current_ek_type][current_criterion]\n",
    "                means = []\n",
    "                stds = []\n",
    "                current_labels = []\n",
    "                current_colors = []\n",
    "\n",
    "                current_indexes = np.arange(len((current_expe.keys())))\n",
    "                is_diff_significant = []\n",
    "                t1 = 0\n",
    "                for teacher_1 in current_expe.keys():\n",
    "                    means.append(current_expe[teacher_1][\"mean\"])\n",
    "                    stds.append(current_expe[teacher_1][\"std\"])\n",
    "                    current_labels.append(teacher_1)\n",
    "                    current_colors.append(per_model_colors[teacher_1])\n",
    "                    t2 = 0\n",
    "                    for teacher_2 in current_expe.keys():\n",
    "                        if t2 > t1:\n",
    "                            if teacher_1 != teacher_2:\n",
    "                                ttest_result = get_simple_welch(current_expe[teacher_1][\"seeds\"], \n",
    "                                                                current_expe[teacher_2][\"seeds\"])\n",
    "                                if ttest_result[1] < 0.05:\n",
    "                                    is_diff_significant.append((t1, t2))\n",
    "                        t2 += 1\n",
    "                    t1 += 1\n",
    "\n",
    "                ax.p1 = plt.bar(current_indexes, means, color=current_colors)\n",
    "#                 ax.errs = plt.errorbar(current_indexes, means, yerr=stds)\n",
    "\n",
    "                k = 0\n",
    "                for significant_diff in is_diff_significant:\n",
    "                    barplot_annotate_brackets(significant_diff[0], significant_diff[1], \n",
    "                                              \"\", current_indexes, means, k, barh=0.01)\n",
    "                    k += 2.2\n",
    "                plt.xticks(current_indexes, current_labels, fontsize=30)\n",
    "                plt.ylim(ymax=100)\n",
    "    \n",
    "    plt.savefig('TeachMyAgent/graphics/{}.png'.format(figname), bbox_inches='tight', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_args_str(dictionary):\n",
    "    args_str = []\n",
    "    for key in dictionary:\n",
    "        args_str.append(\"--{}\".format(key))\n",
    "        if dictionary[key] is not None:\n",
    "            args_str.append(\"{}\".format(dictionary[key]))\n",
    "\n",
    "    return args_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_values(values):\n",
    "    if isinstance(values, np.ndarray):\n",
    "        for i in range(len(values)):\n",
    "            values[i] = round(values[i], 3)\n",
    "    else:\n",
    "        values = round(values, 3)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_to_str(params_dict, line_width=116):\n",
    "    result = str(params_dict)\n",
    "    nb_splits = max(1, len(result) // line_width)\n",
    "    final_result = \"\"\n",
    "    for i in range(nb_splits):\n",
    "        p1 = result[i*line_width:line_width]\n",
    "        p2 = result[(i+1)*line_width:(i+2)*line_width]\n",
    "        final_result = final_result + p1 + \"\\n\" + p2\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_tasks_results(env, env_params_list, env_rewards_list, fig_name, nb_env_test_to_check=None):\n",
    "    nb_env = len(env_params_list) if nb_env_test_to_check is None else nb_env_test_to_check\n",
    "    nb_plots_per_row = 2\n",
    "    nb_rows = math.ceil(nb_env/nb_plots_per_row)\n",
    "    f = plt.figure()\n",
    "    f.set_figwidth(25)\n",
    "    f.set_figheight(6*nb_rows)\n",
    "        \n",
    "    for i in range(nb_env):\n",
    "        fig = plt.subplot(nb_rows, nb_plots_per_row, i+1)\n",
    "        rounded_current_params = {k: round_values(v) for k, v in env_params_list[i].items()}\n",
    "        fig.text(-0.05, 1.03, \n",
    "                 \"Test env nb {0} \\nScore performed: {1} \\nEnv params: {2}\".format(i, env_rewards_list[i], params_to_str(rounded_current_params)), \n",
    "                     ha=\"left\", transform=fig.transAxes)\n",
    "        \n",
    "        env.set_environment(**env_params_list[i])\n",
    "        env.reset()\n",
    "        \n",
    "        plt.imshow(env.render(mode='rgb_array'))\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.savefig('TeachMyAgent/graphics/{}.png'.format(fig_name), bbox_inches='tight', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_test_sets_analysis(dataset_folder, settings, nb_tasks=-1, test_set=None, ep_returns=None):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "    parser = test_policy.get_parser()\n",
    "    parser.add_argument('--expe_name', type=str)\n",
    "    \n",
    "    i = 0\n",
    "    result = {}\n",
    "    for setting in settings:\n",
    "        current_expe_best_seed = best_seeds[setting[\"expe_name\"]]\n",
    "        data_path = os.path.join(dataset_folder, setting[\"expe_name\"], setting[\"expe_name\"] + \"_s\" + str(current_expe_best_seed))\n",
    "        setting[\"fpath\"] = data_path\n",
    "    \n",
    "        args_str = dict_to_args_str(setting)\n",
    "\n",
    "        args = parser.parse_args(args_str)\n",
    "        env_fn, param_bounds, _, _ = EnvironmentArgsHandler.get_object_from_arguments(args)\n",
    "        env = env_fn()\n",
    "        env._SET_RENDERING_VIEWPORT_SIZE(4000, 2000, keep_ratio=True)\n",
    "        \n",
    "        if test_set is None:\n",
    "            test_set_params, rewards = test_policy.load_training_test_set(data_path, order_by_best_rewards=args.bests)\n",
    "        else:\n",
    "            test_set_params = test_policy.load_fixed_test_set(data_path, test_set)\n",
    "            rewards = ep_returns[i]\n",
    "        result[setting[\"expe_name\"]] = [param_dict_to_param_vec(param_bounds, param) for param in test_set_params]\n",
    "        \n",
    "        if nb_tasks == -1:\n",
    "            nb_tasks = len(test_set_params)\n",
    "        \n",
    "        ordering_name = \"\"\n",
    "        if args.bests is None:\n",
    "            ordering_name = \"firsts\"\n",
    "        elif args.bests:\n",
    "            ordering_name = \"top\"\n",
    "        else:\n",
    "            ordering_name = \"worse\"\n",
    "        fig_name = \"{0}_s{1}_{2}test-set-analysis_{3}_{4}\".format(args.expe_name, \n",
    "                                                               current_expe_best_seed, \n",
    "                                                               \"fixed-\" if test_set is not None else \"\",\n",
    "                                                               ordering_name,\n",
    "                                                               nb_tasks)\n",
    "        plot_test_tasks_results(env, test_set_params, rewards, fig_name, nb_env_test_to_check=nb_tasks)\n",
    "        env.close()\n",
    "        i+=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, edge_color=None, face_color=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    covariance = covariance[0:2,0:2]\n",
    "    position = position[0:2]\n",
    "\n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "\n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(2, 3):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs, edgecolor=edge_color, facecolor=face_color))\n",
    "\n",
    "def get_colorbar(cmap, ax):\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = ax.figure\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = fig.colorbar(cmap, cax=cax)\n",
    "    plt.sca(ax)\n",
    "    return cbar; cax\n",
    "\n",
    "def plot_gmm(weights, means, covariances, X=None, ax=None, xlim=[0,1], ylim=[0,1], xlabel='', ylabel='',\n",
    "             bar=True, bar_side='right'):\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    ft_off = 15\n",
    "\n",
    "    ax = ax or plt.gca()\n",
    "    colormap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "    cmap = truncate_colormap(colormap, minval=0.0,maxval=1.0)\n",
    "    for pos, covar, w in zip(means, covariances, weights):\n",
    "        draw_ellipse(pos, covar, alpha=0.6, ax=ax, edge_color=cmap(pos[-1]), face_color=cmap(pos[-1]))\n",
    "\n",
    "    if bar:\n",
    "#         divider = make_axes_locatable(ax)\n",
    "#         cax = divider.append_axes(\"right\", size=\"5%\", pad=0.5)\n",
    "        cax, _ = cbar.make_axes(ax, location=bar_side, shrink=0.8)\n",
    "        cb = cbar.ColorbarBase(cax, cmap=cmap)\n",
    "        cb.set_label('Absolute Learning Progress', fontsize=ft_off + 5)\n",
    "        cax.tick_params(labelsize=ft_off + 0)\n",
    "        cax.yaxis.set_ticks_position(bar_side)\n",
    "        cax.yaxis.set_label_position(bar_side)\n",
    "\n",
    "def generate_stumps_curriculum(dataset_folder, settings, frequency=1, initial_frequency=250000,\n",
    "                           stump_height_dims=[-1, 4], stump_spacing_dims=[-1, 7]):\n",
    "    parser = test_policy.get_parser()\n",
    "    parser.add_argument('--expe_name', type=str)\n",
    "    \n",
    "    result = {}\n",
    "    for setting in settings:\n",
    "        current_expe_best_seed = best_seeds[setting[\"expe_name\"]]\n",
    "        \n",
    "        args_str = dict_to_args_str(setting)\n",
    "        args = parser.parse_args(args_str)\n",
    "        _, param_bounds, _, _ = EnvironmentArgsHandler.get_object_from_arguments(args)\n",
    "        \n",
    "        current_label = labels[setting[\"expe_name\"]]\n",
    "        data = models_saves[setting[\"expe_name\"]]['data'][current_expe_best_seed]\n",
    "        task_samples = data[\"periodical_samples\"]\n",
    "        associated_infos = data[\"periodical_infos\"]\n",
    "        filenames = []\n",
    "        \n",
    "        for i in range(0, len(task_samples), frequency):\n",
    "            if len(task_samples[i]) == 0:\n",
    "                continue\n",
    "                \n",
    "            f, ax = plt.subplots(1,1,figsize=(20,20))\n",
    "            current_data = task_samples[i]\n",
    "            infos = associated_infos[i]\n",
    "            hue = None\n",
    "            ax.set_ylabel('stump_spacing', fontsize=25)\n",
    "            ax.set_xlabel('stump_height', fontsize=25)\n",
    "            plt.xticks(fontsize=18)\n",
    "            plt.yticks(fontsize=18)\n",
    "            plt.xlim(stump_height_dims[0], stump_height_dims[1])\n",
    "            plt.ylim(stump_spacing_dims[0], stump_spacing_dims[1])\n",
    "            set_legend = lambda: None\n",
    "            bk_index = infos[0][\"bk_index\"]\n",
    "            if current_label in [\"ALP-GMM\", \"Covar-GMM\"]:\n",
    "                if bk_index > 0:\n",
    "                    plot_gmm(data[\"weights\"][bk_index], data[\"means\"][bk_index], data[\"covariances\"][bk_index], \n",
    "                             ax=ax, xlim=stump_height_dims, ylim=stump_spacing_dims)\n",
    "            elif current_label == \"Self-Paced\":\n",
    "                draw_ellipse(data[\"mean\"][bk_index], data[\"covariance\"][bk_index], ax=ax, alpha=0.5)\n",
    "            elif current_label == \"ADR\":\n",
    "                x1 = data[\"task_space\"][bk_index][0][0]\n",
    "                x2 = data[\"task_space\"][bk_index][1][0]\n",
    "                y1 = data[\"task_space\"][bk_index][0][1]\n",
    "                y2 = data[\"task_space\"][bk_index][1][1]\n",
    "                ax.add_patch(Rectangle((x1, y1), x2-x1, y2-y1, alpha=0.5))\n",
    "            elif current_label == \"Setter-Solver\":\n",
    "                set_legend = lambda: ax.legend(title=\"Feasibility\", fontsize=25)\n",
    "                hue = [_info[\"task_infos\"][0][0] for _info in infos]\n",
    "            elif current_label == \"RIAC\":\n",
    "#                 for box in data[\"all_boxes\"][bk_index]:\n",
    "#                     ax.add_patch(Rectangle((box.low[0], box.high[1]), \n",
    "#                                            box.low[1]-box.low[0], \n",
    "#                                            box.high[1]-box.high[0], \n",
    "#                                            alpha=0.5, color=))     \n",
    "                set_legend = lambda: ax.legend(title=\"Region ALP\", fontsize=25)\n",
    "                hue = [data[\"all_alps\"][bk_index][_info[\"task_infos\"]] for _info in infos]\n",
    "            \n",
    "            g = sns.scatterplot(x=current_data[:, 0], y=current_data[:, 1], ax=ax, hue=hue, s=100)\n",
    "            legend = set_legend()\n",
    "            if legend is not None:\n",
    "                legend.get_title().set_fontsize('25')\n",
    "                for legobj in legend.legendHandles:\n",
    "                    legobj.set_linewidth(5.0)\n",
    "            f_name = \"ACL_bench/graphics/gifs/scatter_{}.png\".format(i)\n",
    "            plt.suptitle('Step {}'.format(math.ceil(initial_frequency/frequency) * i), fontsize=25)\n",
    "            plt.savefig(f_name, bbox_inches='tight')\n",
    "            plt.close(f)\n",
    "            filenames.append(f_name)\n",
    "        \n",
    "        images = []\n",
    "        for filename in filenames:\n",
    "            images.append(imageio.imread(filename))\n",
    "        imageio.mimsave('TeachMyAgent/graphics/{}.gif'.format(setting[\"expe_name\"] + \"_\" + str(current_expe_best_seed)), images, duration=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parkour_curriculum(dataset_folder, settings, frequency=1, initial_frequency=250000):\n",
    "    parser = test_policy.get_parser()\n",
    "    parser.add_argument('--expe_name', type=str)\n",
    "    \n",
    "    result = {}\n",
    "    for setting in settings:\n",
    "        current_expe_best_seed = best_seeds[setting[\"expe_name\"]]\n",
    "        data_path = os.path.join(dataset_folder, setting[\"expe_name\"], setting[\"expe_name\"] + \"_s\" + str(current_expe_best_seed))\n",
    "        setting[\"fpath\"] = data_path\n",
    "    \n",
    "        args_str = dict_to_args_str(setting)\n",
    "\n",
    "        args = parser.parse_args(args_str)\n",
    "        env_fn, param_bounds, _, _ = EnvironmentArgsHandler.get_object_from_arguments(args)\n",
    "        env = env_fn()\n",
    "        env._SET_RENDERING_VIEWPORT_SIZE(4000, 2000, keep_ratio=True)\n",
    "        \n",
    "        fig_name = \"{0}_s{1}_curriculum-analysis\".format(args.expe_name, \n",
    "                                                               current_expe_best_seed)\n",
    "        data = models_saves[setting[\"expe_name\"]]['data'][current_expe_best_seed]\n",
    "        tasks = data[\"periodical_samples\"]\n",
    "        associated_infos = data[\"periodical_infos\"]\n",
    "        \n",
    "        nb_env = math.ceil(len(tasks) / frequency)\n",
    "        \n",
    "        filenames = []\n",
    "        \n",
    "        for i in range(0, nb_env-1, frequency):\n",
    "            if len(tasks[i]) == 0:\n",
    "                continue\n",
    "            current_tasks = tasks[i]\n",
    "            current_infos = associated_infos[i]\n",
    "            index = random.randint(0, len(current_tasks)-1)\n",
    "            task = param_vec_to_param_dict(param_bounds, current_tasks[index])\n",
    "            associated_info = current_infos[index]\n",
    "            f, ax = plt.subplots(1,1,figsize=(12,10))\n",
    "\n",
    "            env.set_environment(**task)\n",
    "            env.reset()\n",
    "\n",
    "            plt.imshow(env.render(mode='rgb_array'))\n",
    "            plt.axis('off')\n",
    "            f_name = \"ACL_bench/graphics/gifs/{}_{}.png\".format(fig_name, i)\n",
    "            plt.suptitle('Step {}'.format(math.ceil(initial_frequency/frequency) * i), fontsize=20)\n",
    "            plt.savefig(f_name, bbox_inches='tight')\n",
    "            plt.close(f)\n",
    "            filenames.append(f_name)\n",
    "\n",
    "        images = []\n",
    "        for filename in filenames:\n",
    "            images.append(imageio.imread(filename))\n",
    "        imageio.mimsave('TeachMyAgent/graphics/{}.gif'.format(setting[\"expe_name\"] + \"_\" + str(current_expe_best_seed)), images, duration=0.3)\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy_perf(dataset_folder, settings):\n",
    "    parser = test_policy.get_parser()\n",
    "    parser.add_argument('--expe_name', type=str)\n",
    "    ep_returns = []\n",
    "    \n",
    "    for setting in settings:\n",
    "        current_expe_best_seed = best_seeds[setting[\"expe_name\"]]\n",
    "        data_path = os.path.join(dataset_folder, setting[\"expe_name\"], setting[\"expe_name\"] + \"_s\" + str(current_expe_best_seed))\n",
    "        setting[\"fpath\"] = data_path\n",
    "        setting[\"record\"] = False\n",
    "        setting[\"norender\"] = None\n",
    "    \n",
    "        args_str = dict_to_args_str(setting)\n",
    "\n",
    "        args = parser.parse_args(args_str)\n",
    "        ep_returns.append(test_policy.main(args))\n",
    "    return ep_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_policy(dataset_folder, settings):\n",
    "    parser = test_policy.get_parser()\n",
    "    parser.add_argument('--expe_name', type=str)\n",
    "    \n",
    "    for setting in settings:\n",
    "        current_expe_best_seed = best_seeds[setting[\"expe_name\"]]\n",
    "        data_path = os.path.join(dataset_folder, setting[\"expe_name\"], setting[\"expe_name\"] + \"_s\" + str(current_expe_best_seed))\n",
    "        setting[\"fpath\"] = data_path\n",
    "        setting[\"record\"] = True\n",
    "        setting[\"recording_path\"] = os.path.join(setting[\"recording_path\"], setting[\"expe_name\"] + \"_s\" + str(current_expe_best_seed))\n",
    "    \n",
    "        args_str = dict_to_args_str(setting)\n",
    "\n",
    "        args = parser.parse_args(args_str)\n",
    "        test_policy.main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the function calls below to plot your experiments. You can use regex-like patterns with some modifications:\n",
    "- `*` means anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## % Nb mastered Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_curves(\"benchmark_parkour_(ADR|ALP-GMM|Covar-GMM|RIAC|Random|Setter-Solver|Self-Paced|GoalGAN)$\",leg_size=40, y_max=32, plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, welch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_curves(\"*08-01_test_stump_tracks_Self-Paced*\", plot_type=\"all_and_median\", allow_different_sizes=True, x_max=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_Covar-GMM_criteria_5*_no_\", x_max=20, plot_type=\"shade_se\", metric=\"evaluation return\", allow_different_sizes=True, y_min=-300, y_max=310, welch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_curves(\"*subset_parkour_climbing_easy_parkour_1*Random\", leg_size=30, x_max=10, welch=False, allow_different_sizes=True, plot_type=\"shade_se\", metric=\"training return\", y_min=-200, y_max=310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_all_comparisons(\"benchmark_parkour_(ADR|ALP-GMM|Covar-GMM|RIAC|Random|Setter-Solver|Self-Paced|GoalGAN)$\", welch_p_threshold=0.01, metric=\"nb_mastered\", y_min=0, y_max=100, allow_different_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_profile_chart(baseline_teacher=\"Random\", tick_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_profile_chart(baseline_teacher=\"Random\", tick_step=1, timestep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_comparison_bars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test tasks analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the settings below to load the best seed onf one of your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TO CHANGE ####\n",
    "settings = [\n",
    "    {\n",
    "        \"env\": \"parametric-continuous-parkour-v0\",\n",
    "        \"walker_type\": \"climbing_profile_chimpanzee\",\n",
    "        \"bests\": True, # Whether the results should be ordered by performance (best performance first)\n",
    "        \"lidars_type\": \"up\", # Use 'up' for climbers, 'down' for walkers and 'full' for swimmers\n",
    "        \"deterministic\": None, # Leave thins to None\n",
    "        \"len\": 2000, # Leave this to 2000\n",
    "        \"expe_name\" : \"10-08_subset_parkour_climbing_easy_parkour_1_teacher_Random\",\n",
    "        \"episode_ids\": \"0\", # -1 means all the episodes\n",
    "        \"recording_path\": \"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_sets = perform_test_sets_analysis(data_folder, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "record_policy(data_folder, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curriculum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parkour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_parkour_curriculum(data_folder, settings = [\n",
    "    {\n",
    "        \"env\": \"parametric-continuous-parkour-v0\",\n",
    "        \"walker_type\": \"old_classic_bipedal\",\n",
    "        \"lidars_type\": \"down\",\n",
    "        \"expe_name\" : \"14-12_benchmark_parkour_Setter-Solver_walker_type_old_classic_bipedal\",\n",
    "    }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_parkour_curriculum(data_folder, settings = [\n",
    "    {\n",
    "        \"env\": \"parametric-continuous-parkour-v0\",\n",
    "        \"walker_type\": \"climbing_profile_chimpanzee\",\n",
    "        \"lidars_type\": \"up\",\n",
    "        \"expe_name\" : \"14-12_benchmark_parkour_Setter-Solver_walker_type_climbing_profile_chimpanzee\",\n",
    "    }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_parkour_curriculum(data_folder, settings = [\n",
    "    {\n",
    "        \"env\": \"parametric-continuous-parkour-v0\",\n",
    "        \"walker_type\": \"fish\",\n",
    "        \"lidars_type\": \"full\",\n",
    "        \"expe_name\" : \"14-12_benchmark_parkour_Setter-Solver_walker_type_fish\",\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stump Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stumps_curriculum(data_folder, settings = [\n",
    "    {\n",
    "        \"expe_name\" : \"14-12_profiling_benchmark_stumps_Setter-Solver_criteria_1_allow_expert_knowledge_maximal\",\n",
    "    }], stump_height_dims=[-1, 10], stump_spacing_dims=[-1, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stumps_curriculum(data_folder, settings = [\n",
    "    {\n",
    "        \"expe_name\" : \"14-12_profiling_benchmark_stumps_Setter-Solver_criteria_2_allow_expert_knowledge_maximal\",\n",
    "    }], stump_height_dims=[-4, 4], stump_spacing_dims=[-1, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stumps_curriculum(data_folder, settings = [\n",
    "    {\n",
    "        \"expe_name\" : \"14-12_profiling_benchmark_stumps_Setter-Solver_criteria_3_allow_expert_knowledge_maximal\",\n",
    "    }], stump_height_dims=[-1, 4], stump_spacing_dims=[-1, 7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
